1. @[toc](清华青年AI自强作业hw3_2：前向传播和反向传播实战)

    ![在这里插入图片描述](https://img-blog.csdnimg.cn/c3c2e4b7dae44bef909a63d75eeb227f.jpeg)

    > 一起学AI系列博客：[目录索引](https://blog.csdn.net/qq_17256689/article/details/130910780)

    

    前向传播和反向传播的公式理解化用于作业hw3_2中：：用NN网络拟合小姐姐喜好分类

    - 完成前向传播、反向传播算法的Python实现
    - 完成课堂的网络模型搭建，并实现喜好分类训练集精度100%

    ## 项目分析

    ---

    用NN模型拟合小姐姐喜好分类的实质：

    - 将前向计算的过程按公式转化为代码
    - 将反向传递的过程按公式转化为代码
    - 根据反向传递的误差更新每层神经元参数
    - 设计迭代次数及停止loss条件
    - 按需求计算一次loss值（代价大，尽量少算）
    - 计算参数更新后，现有训练集的预测准确率
    - 参数更新停止时，计算预测集的新数据预测准确率

    过程：

    1. 用四层网络替代原四个神经元一层网络线性回归模型，测试喜好划分精度。体会过拟合与泛化的问题。
    2. 用四个神经元一层网络模型，去测试数字分类的精度。体会模型的重要性。

    注意：

    1. 前向计算中间结果需一直保存，直到反向传递计算更新参数后失效
    2. 误差更新等公式是根据loss函数计算得来

    ## 实现过程

    ----

    FP前向传播计算公式与例图：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/56c843da84484388b9e14751afade37a.png)


    BP反向传播误差计算参考公式和例图：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/46789c3992e94b6fa2fdccb3c17822db.png)


    计算Dij/Wij的公式：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/486ceecec75f40349793088f5a843f40.png)
    
    如想对反向传播计算过程有深入理解，可以参考：[“反向传播算法”过程及公式推导（超直观好懂的Backpropagation）
    ](https://blog.csdn.net/ft_sunshine/article/details/90221691)


    ## 参数分析


    ----
    
    各层参数维度分析，已知信息：
    
    - 输入层不需要扩展列，中间隐藏层需要扩展列表征偏置
    - 样本数50个
    
    **前向传播过程**
    
    每层神经元经过激活函数后输出：
    
    - a1: (3, 50)，3表示该层有3个神经元，每列表示对应第i个样本的输出结果
    - a2: (6, 50)，6表示6个神经元，含偏置神经元，默认输出无权重对应连接
    - a3: (4, 50)
    - a4: (1, 50)
    
    经过权重调节后的输出：
    
    - z1=a1
    - z2: (5, 50)，6-1减去偏置神经元
    - z3: (3, 50)，4-1减去偏置神经元
    - z4: (1, 50)，无神经元
    
    同一层z的维度与a一致，运算经过点乘实现非线性变换。偏置神经元无z->a非线性变换，公式举例：`z2 = w1 * a1`。
    
    每层神经元相应连接权重：主要跟前后神经元个数有关系
    
    - W1: (5, 3)，5表示权重连接下一层共有5个神经元（下层含偏置神经元1个，则下层共有5+1个神经元），3表示当前层神经元个数
    - W2: (3, 5)
    - W3: (1, 3)
    
    **反向传播过程：**
    
    每层神经元的误差
    
    - delta4: (1, 50)，1为最后一层神经元个数，50为样本数
    - delta3: (3, 50)，去掉了偏置神经元
    - delta2: (5, 50)，去掉了偏置神经元
    
    各层神经元所有样本的累积误差
    
    - bigDelta1: (5, 3)，5表示权重连接下一层共有5个神经元（下层含偏置神经元1个，则下层共有5+1个神经元），3表示当前层神经元个数
    - bigDelta2: (3, 6)
    - bigDelta3: (1, 4)，含偏置神经元
    
    各层神经元所有样本的均值误差
    
    - D3与W3维度一致
    - D2与W2
    - D1与W1
    
    ## 拟合结果
    
    ---
    
    显然成功过拟合了，说明用这个模型来拟合这点数据规模，有点杀鸡用牛刀了。
    
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/a42287422646453da37d3e4304a5fe63.png)
    
    **问题解决：**
    
    1. 为啥predict函数和precision函数输入的维度不同？
        - 解决：两者没本质区别，precision之所以没有扩展偏置神经元，是因为输入用的是predict扩展后的X_m输入。predict的输入为X，需要扩展偏置神经元。
    
    ## 相关链接
    
    ---
    
    1. 文科生都能零基础学AI？清华这门免费课程让我信了，[link](https://blog.csdn.net/qq_17256689/article/details/123290351)
    2. 清华青年AI自强作业2：线性回归预测，[link](https://blog.csdn.net/qq_17256689/article/details/124435599)